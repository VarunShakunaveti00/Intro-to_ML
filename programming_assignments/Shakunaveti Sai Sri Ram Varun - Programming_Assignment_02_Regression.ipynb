{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " <center> <h1> <b> Pattern Recognition and Machine Learning (EE2802 - AI2000) </b> </h1> </center>\n",
        "\n",
        "<b> Programming Assignment 02 - Regression : </b> Welcome to the programming assignment (PA) on regression. This programing assignment focuses on understanding the basic concepts of linear regression.\n",
        "\n",
        "\n",
        "<b> Instructions </b>\n",
        "1. Plagiarism is strictly prohibited.\n",
        "2. Delayed submissions are not accepted\n",
        "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n"
      ],
      "metadata": {
        "id": "TBV8MLPBSv1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h2> <b> Understanding Basic Concepts </b> </h2> </center>\n",
        "\n",
        "\n",
        "<b> Part - (1) :  Understanding Error Surfaces </b>\n",
        "\n",
        "According to www.geogebra.org, the relationship between human height (in\n",
        "inches) and weight (in pounds) is given by <br>\n",
        "<center> $t = 3.86x - 110.42$ </center>\n",
        "\n",
        "(a) Generate 25 meaningful data points from this relationship, mimicking a\n",
        "noisy sensor, where the noise follows a zero mean Gaussian with a variance\n",
        "of 20. Plot the scatter plot of the data. <br>\n",
        "(b) Now, we need to estimate the above relationship from the noisy data\n",
        "generated in (a) by fitting a line, i.e $\\hat{t} = y(x,w) = w_{0} + w_{1}x$. Let us use least squares criterion discussed in the class to estimate the parameters $w_{0}$ and $w_{1}$. Generate and plot the error surface $J(w_{0},w_{1})$ associated with this approach. Locate the minimum on this error surface.<br>\n",
        "\n",
        "(c) Estimate the parameters using least squares approach, and compare them\n",
        "with the desired values.\n",
        "<center> $\\textbf{w}_{opt} = (\\textbf{X}^{T}\\textbf{X})^{-1}\\textbf{X}^{T}\\textbf{t}$</center>\n",
        "\n",
        "(d) Report all your observations"
      ],
      "metadata": {
        "id": "etb5RPr5_qAw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTeUZYn2vy0H"
      },
      "source": [
        "<b> Part - (2) : Understanding model order and overfitting  </b>\n",
        "\n",
        "(a). Generate  20  data  points  from $t_{n} = sin(2πx_{n}) + e_{n}$, where $x_{n} \\in [0,1] $ and $e_{n} \\thicksim \\mathcal{N} (0,0.1)$ , and divide them into two sets, a training set and a testing set, with each set containing 10 points <br>\n",
        "\n",
        "(b). Fit  an $M^{th}$ degree  polynomial  to  the  training  data  using  least  squares approach, i.e.,\n",
        "<center> $\\hat{t_{n}} = w_{0} + w_{1}x + .... +  w_{m}x^{m} + ... + w_{M}x^{M} $ </center>\n",
        "\n",
        "Use the estimated parameter vector $\\textbf{w}$, to predict the target values in training and testing datasets.  Plot the root mean squared error associated with each dataset, for M=0,1,...,9. Explain your results. <br>\n",
        "\n",
        "(c) Increase the size of the training dataset to 100 points, and repeat (b). <br>\n",
        "\n",
        "(d) Add a $l_{2}$ regularization term to the objective function in (b) and repeat (b) and (c).  Study the affect of Lagrange multiplier λ on the root mean squared error of the training and testing datasets <br>\n",
        "\n",
        "(e) Modify the function in (a) to $t_{n}=5+sin(2πx_{n})+e_{n}$ to study the effect of regularizing the bias coefficient $w_{0}$.\n",
        "\n",
        "(f) Report all your observations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (3) : Understanding the choice of kernel  </b>\n",
        "\n",
        "\n",
        "(a). Generate 100 data points from $t_{n}=sin(2πx_{n})+e_{n}$, where $x_{n} \\in [0 1]$ and $e_{n} \\thicksim \\mathcal{N}(0,0.1)$, and divide them into two sets, a training set and a testing test each containing 50 points.  Fit an $M^{th}$ degree polynomial using polynomial,Gaussian and sigmoidal kernels, and study the goodness of fit in each case, for different model orders M\n",
        "\n",
        "(b). Repeat (a) by modifying the target function to <br>\n",
        "<center> $t_{n} = $ $\\begin{cases}\n",
        " \\text{sinusoid} + e_{n} , \\;\\; where \\;\\; x  \\in [0,1) \\\\\n",
        " \\text{triangle} + e_{n} , \\;\\; where \\;\\; x  \\in [1,2) \\\\\n",
        " \\text{Gaussian} + e_{n} , \\;\\; where \\;\\; x  \\in [2,3) \\\\\n",
        "\\end{cases}$ </center>\n",
        "\n",
        "Clearly discuss your observations/results for each of the three kernels.\n",
        "\n",
        "(c). Report all your observations"
      ],
      "metadata": {
        "id": "UP_gGkTIAupD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (4) : Understanding online training  </b>\n",
        "\n",
        "(a). Repeat 3(a) and 3(b) using stochastic gradient descent for weight update.Study the effect of step size η on convergence of the weights, and compare them to those obtained using closed form expressions in 3.  Plot the mse as a  function  of  iterations.\n",
        "\n",
        "(b). Study the effect of batch size on the speed of convergence\n",
        "\n",
        "(c). Report all your observations"
      ],
      "metadata": {
        "id": "xLDBH_w-VoC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (5) : Understanding bias-variance trade-off  </b>\n",
        "\n",
        "(a). Generate L=100 datasets of noisy sinusoidal data, each having N=25  datapoints. For each dataset, fit a $M=25^{th}$ order linear regression model consisting of 24 Gaussian basis functions and one bias parameter.  Use regularized least squares, governed by the parameter λ, to estimate the parameters $\\textbf{w}$. Illustrate the concept of bias and variance using these 100 different parameter fits.\n",
        "1.   Chose three different regularization coefficeints (low,middle and high)\n",
        "2. For every regularization coefficient, produce two plots: one displaying 100 estimated curves, and the other showing the mean of the estimated curves alongside the original function.\n",
        "2. For three regularization coefficients, you should have a total of six plots, meaning two plots for each regularization.\n",
        "3. Using the six plots above, describe the bias-variance trade-off.\n",
        "\n",
        "\n",
        "(b). Report all your observations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8XlziBKcVwP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Part - (6) : Understanding\n",
        "Maximum a Posteriori (MAP) estimate  </b>\n",
        "\n",
        "(a). Generate 100 noisy data points of a sinusoid. Fit a $20^{th}$  order  linear regression  model  with  Gaussian  basis  functions. Starting from a standard normal prior, update the statistics of the posterior density of the parameters using Bayesian sequential updates.\n",
        "\n",
        "(b). Sample a parameter vector from the posterior distribution, and obtain the curve fit for this realization. Repeat this for several times, and estimate the average of these curve fits, and compare it with the original sinusoid\n",
        "\n",
        "(c). Use the posterior distribution of the parameters to evaluate the predictive distribution of target $p(t_{0}/x_{0},X,t)$, and plot it for different number of training data points, as discussed in the class.\n",
        "\n",
        "(d). Report all your observations"
      ],
      "metadata": {
        "id": "115hZVaX9SS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Understanding Error Surface\n",
        "#All imports\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "########################################\n",
        "#Generate meaningfull data\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Plot scatter plot of data\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Weight estimation through error surface, i.e., empirically locate the minima of error surface\n",
        "########################################\n",
        "#Sample a bunch of w's around w_opt and compute the associated error\n",
        "\n",
        "\n",
        "\n",
        "#Compute the error\n",
        "def Error(w,t,x): #inputs : 1)weight 2)data i.e (t,x)\n",
        "    #Estimate the target\n",
        "\n",
        "    #Compute and return the error\n",
        "    return error\n",
        "\n",
        "\n",
        "#Plot 3D error surface and the corresponding contour plots\n",
        "#Error surface is a function of w0 and w1\n",
        "\n",
        "\n",
        "\n",
        "#Locate the minima of the error surface\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Least squares approach to estimate the weights\n",
        "########################################\n",
        "#Complete the below linear regression function\n",
        "def LinearRegression(x,t): #inputs : 1)input data i.e (x). 2)target i.e (t)\n",
        "\n",
        "    return w_opt\n",
        "\n",
        "\n",
        "#Estimate optimal weight's using \"LinearRegression\" function\n",
        "\n",
        "\n",
        "\n",
        "#Estimate the targets using the input x and the estimated weights\n",
        "\n",
        "\n",
        "\n",
        "#Plot the estimated line on top of the above scatter plot\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Compare the estimated weight's using least squares approach with the error surface approach\n",
        "########################################\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iaH6TnNVaF6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "T8NY_9Lf8QEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Understanding model order and overfitting\n",
        "########################################\n",
        "#Generate 20 data points\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "#Obtain train and test splits\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Fitting Mth degree polynomial using least squares approach\n",
        "########################################\n",
        "#Complete the function\n",
        "def PolynomialFit(X_train,Y_train,M,lamda): #(training data, trining targets, Model order, Regularization coefficient)\n",
        "    #Transform the data using polynomial kernel\n",
        "\n",
        "    #Find Pseudo inverse solution\n",
        "\n",
        "    #return the weight vector\n",
        "    return w_opt\n",
        "\n",
        "#Complete the function\n",
        "def PolynomialPred(w_est,X_train,X_test): #(weight,training data, testing data, training targets, testing targets)\n",
        "    #Estimate the targets for both training and testing data\n",
        "\n",
        "\n",
        "    #Return training and testing predictions\n",
        "    return TrainError,TestError\n",
        "\n",
        "#Complete the function\n",
        "def PolynomialPred_Error(w_est,X_train,X_test,Y_train,Y_test): #(weight,training data, testing data, training targets, testing targets)\n",
        "    #Estimate the targets for both training and testing data\n",
        "\n",
        "\n",
        "    #Return training and testing error\n",
        "    return TrainError,TestError\n",
        "\n",
        "#Iterate through range of M values\n",
        "M_range=list(range(10))\n",
        "TrError = []\n",
        "TeError = []\n",
        "for M in M_range:\n",
        "    #Fit Mth order polynomial i.e estimate optimal w. Use the function \"PolynomialFit\"\n",
        "\n",
        "\n",
        "    #Predict training and testing targets\n",
        "\n",
        "\n",
        "    #Predict errors on both training and testing data using estimated w. Use the function \"PolynomialPred_Error\"\n",
        "\n",
        "\n",
        "    #Store them for plotting\n",
        "\n",
        "\n",
        "#Plot training and testing estimates alogwith the original targets\n",
        "\n",
        "\n",
        "\n",
        "#Plot training error vs polynomial order, and testing error vs polynomial order\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Increase the size of training data set to 100 points and repeat the experiments\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Effect of regularization\n",
        "########################################\n",
        "#Consider a set of lambda's. For example: lamdas = [0, 1e-7 , 1e-4, 1e-2, 1]\n",
        "#Repeat the experiments, i.e., plot the prediction and error in predictions with respect to model order. Contrast these results with those obtained without regularization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Effect of bias regularization\n",
        "########################################\n",
        "#Modify the function i.e include bias\n",
        "#Generate data\n",
        "\n",
        "\n",
        "\n",
        "#Estimate the polynomial with and without regularization constraint\n",
        "\n",
        "\n",
        "\n",
        "#Compare the two estimated polynomials and report the observations\n",
        "\n"
      ],
      "metadata": {
        "id": "WizAlOHEcXRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "jgU-jHTb8OBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Understanding the choice of kernel\n",
        "########################################\n",
        "#Generate 100 data points\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "#Obtian train and test splits\n",
        "#Take even samples for training and odd samples for testing\n",
        "\n",
        "\n",
        "\n",
        "#Function to estimate the parameters\n",
        "def KernelRegressionFit(X_train,Y_train,kernelType,M,lamda): #(training data, training targets, type of kernel, regularization coefficient)\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "    #print(X_train.shape)\n",
        "    #X_train = np.reshape(X_train, (1,-1))\n",
        "    #Y_train = np.reshape(Y_train, (1,-1))\n",
        "    X = []\n",
        "    if kernelType=='polynomial':\n",
        "      #Use polynomial kernel to transform the data\n",
        "\n",
        "    if kernelType=='gaussian':\n",
        "      #Use Gaussian kernel to transform the data\n",
        "\n",
        "    if kernelType=='sigmoidal':\n",
        "      #Use Sigmoid kernel to transform the data\n",
        "\n",
        "    #Estimate weights using Pseudo iverse solution\n",
        "\n",
        "\n",
        "    #Return the estimated weights\n",
        "    return w_opt\n",
        "\n",
        "#Function to compute the training and testing errors from the current weight estimates\n",
        "def KernelRegressionPred_Error(w_est,X_train,Y_train,X_test,Y_test,kernelType):\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "    X_tr = []\n",
        "    X_te = []\n",
        "    #X_train = np.reshape(X_train, (1,-1))\n",
        "    #Y_train = np.reshape(Y_train, (1,-1))\n",
        "    #X_test = np.reshape(X_test, (1,-1))\n",
        "    #Y_test = np.reshape(Y_test, (1,-1))\n",
        "    M = len(w_est)-1\n",
        "    if kernelType=='polynomial':\n",
        "      #Use polynomial kernel to transform the data\n",
        "\n",
        "    if kernelType=='gaussian':\n",
        "      #Use Gaussian kernel to transform the data\n",
        "\n",
        "    if kernelType=='sigmoidal':\n",
        "      #Use Sigmoid kernel to transform the data\n",
        "\n",
        "\n",
        "    #Estimate training and testing targets\n",
        "\n",
        "\n",
        "    #Compute and return the training and testing errors\n",
        "    return TrainError,TestError\n",
        "\n",
        "\n",
        "#Iterate through range of M values\n",
        "M_range=list(range(10))\n",
        "\n",
        "polynomial_tr_error = []\n",
        "polynomial_te_error = []\n",
        "gaussian_tr_error = []\n",
        "gaussian_te_error = []\n",
        "sigmoid_tr_error = []\n",
        "sigmoid_te_error = []\n",
        "\n",
        "for M in M_range:\n",
        "    #Fit Mth order polynomial using three kernels i.e {Polynomial,Gaussian,Sigmoid}\n",
        "\n",
        "\n",
        "\n",
        "    #Predict training and testing targets using estimated w\n",
        "\n",
        "\n",
        "    #Predict errors on both training and testing data using estimated w\n",
        "\n",
        "\n",
        "\n",
        "    #Store them for plotting\n",
        "\n",
        "\n",
        "#Plot the predicted training and testing targets alongside the original targets for various model orders and all three different kernels.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Plot training error vs polynomial order and testing error vs polynomial order for all the three different kernels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Repeat the experiments by changing target function\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5X9dVqUdx-ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mwbo51Fy8MQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Understanding online training\n",
        "########################################\n",
        "#Repeat 3(a) and 3(b) using stochastic gradient descent for weight update ( plot required results )\n",
        "########################################\n",
        "\n",
        "def ErrorPred(w_est,X_train,Y_train,X_test,Y_test,kernelType): #(estimated weight, training data, training targets, testing data, testing targets, type of the kernel )\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "    X_tr = []\n",
        "    X_te = []\n",
        "    M = len(w_est)-1\n",
        "    if kernelType=='polynomial':\n",
        "      #Use polynomial kernel to transform the data\n",
        "\n",
        "    if kernelType=='gaussian':\n",
        "      #Use Gaussian kernel to transofrm the data\n",
        "\n",
        "    if kernelType=='sigmoidal':\n",
        "      #Use sigmoidal kernel to transform the data\n",
        "\n",
        "    #Compute and return the train and test errors\n",
        "    return TrainError, TestError\n",
        "\n",
        "\n",
        "def OnlineTraining(X_train,Y_train,X_test, Y_test, kernelType,M,Epochs,BatchSize,stepSize): #(training data, training targets, testing data, testing targets, tupe of the kernel, order of the mode, Number of epochs, Batch size, Step size)\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "    #Initialize the weights\n",
        "\n",
        "\n",
        "    #Initialize the necessary variables\n",
        "    #Iterate through epochs\n",
        "\n",
        "    epochs = range(Epochs)\n",
        "    for epoch in epochs:\n",
        "        #Compute the train and test errors using the current weights\n",
        "        tr_err, te_err = ErrorPred(weights, X_train, Y_train, X_test, Y_test, kernelType)\n",
        "        #Store training and testing errors for plotting\n",
        "\n",
        "        #Shuffle the data\n",
        "\n",
        "        #Iterate through the batches\n",
        "        for batch in range(batches):\n",
        "            #Initialize the necessary variables\n",
        "            #Get a batch of data\n",
        "\n",
        "            #Iterate through the data points of obtained batch\n",
        "            for n in range(len(data_b)):\n",
        "                #Obtain kernel representation\n",
        "                X_tr = []\n",
        "                if kernelType=='polynomial':\n",
        "\n",
        "                if kernelType=='gaussian':\n",
        "\n",
        "                if kernelType=='sigmoidal':\n",
        "\n",
        "                #Compute the gradient of weight's\n",
        "\n",
        "                #Compute the running mean of the weights gradients for the batch update\n",
        "\n",
        "            #Update the weights using mean gradient, consider using reasonable stepSize\n",
        "\n",
        "    #Plot training and testing error across the epochs\n",
        "\n",
        "\n",
        "    #Return the estimated weights\n",
        "    return weights\n",
        "\n",
        "def OnlinePred(w_est,X_train,X_test,kernelType): #(estimated weights, training data, testing data, type of the kernel )\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "    #Initialize the required variables\n",
        "\n",
        "    #Obtain kernel representations\n",
        "    if kernelType=='polynomial':\n",
        "\n",
        "    if kernelType=='gaussian':\n",
        "\n",
        "    if kernelType=='sigmoidal':\n",
        "\n",
        "    #Compute and return the training and testing target estimates\n",
        "    return Y_tr_error,Y_te_error\n",
        "\n",
        "def OnlinePred_Error(w_est,X_train,Y_train,X_test,Y_test,kernelType): #(estimated weights, training data, training targets, testing data, testing targets, type of the kernel )\n",
        "    #kernelType : {Polynomial,Gaussian,Sigmoid}\n",
        "    #Initialize the required variables\n",
        "\n",
        "    #Obtain kernel representations\n",
        "    if kernelType=='polynomial':\n",
        "\n",
        "    if kernelType=='gaussian':\n",
        "\n",
        "    if kernelType=='sigmoidal':\n",
        "\n",
        "    #Compute and return the training and testing errors\n",
        "    return Y_tr_error,Y_te_error\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Repeat 3a with online training\n",
        "##################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################################################\n",
        "#Repeat 3b with online training\n",
        "##################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Study the effect of stepSize on the convergence of weights ( plot required results )\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Study the effect of batchsize on the speed of convergence ( plot required results )\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wDIXv_qX01_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "sMtw7C-G8KOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Understanding the bias-variance trade-off\n",
        "########################################\n",
        "#Generate 100 data sets of noisy sinusoidal data\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Use regularized least squares to estimate w\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Illustrate the concept of Bias-Variance trade off\n",
        "########################################\n",
        "#1. Chose three different regularization coefficeints (low,middle and high)\n",
        "#2. For every regularization coefficient, produce two plots: one displaying 100 estimated curves, and the other showing the mean of the estimated curves alongside the original function.\n",
        "#3. For three regularization coefficients, you should have a total of six plots, meaning two plots for each regularization.\n",
        "#4. Using the six plots above, describe the bias-variance trade-off.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r9XEzH9X4uKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "_FB3FW0c8GHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Understanding MAP estimate\n",
        "########################################\n",
        "#Generate 100 data sets of noisy sinusoidal data\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Update the statistics of posterior density\n",
        "########################################\n",
        "#Initialie the parameters for standard normal prior\n",
        "\n",
        "\n",
        "#Iterate through the data points and update the stats of posterior density\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Sample weight vector from posterior distribution. Estimate the curve, repeat the procedure for 100 times and get the avg fit\n",
        "########################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################\n",
        "#Predictive distribution analysis\n",
        "########################################\n",
        "#Predictive distribution analysis through sampling\n",
        "#Iterate through data points and sample weight vectors when partial data points are seen, and plot the curves\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Predictive distribution analysis through variance\n",
        "#Iterate through data points and obtain necessary plots as discussed in the class\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XotE4gBQ5YhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3.\n",
        "\n"
      ],
      "metadata": {
        "id": "mHPFBB5L8FTx"
      }
    }
  ]
}