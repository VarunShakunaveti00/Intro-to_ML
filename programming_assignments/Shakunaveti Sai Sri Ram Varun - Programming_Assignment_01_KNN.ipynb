{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVmT47LOBy6i"
      },
      "source": [
        "\n",
        " <center> <h1> <b> Pattern Recognition and Machine Learning (EE2802 - AI2000) </b> </h1> </center>\n",
        "\n",
        "<b> Programming Assignment 01 - KNN : </b> Welcome to the programming assignment (PA) on k-nearest neighbors (KNN) classification. Throughout this PA, you will explore the k-NN algorithm, a versatile and intuitive method for tackling classification and regression challenges. Specifically, this assignment aims to enhance your understanding of the KNN classification algorithm. In this PA, we expect you to implement and experiment with the KNN classifier to understand how variations in 'k' and distance metrics influence classification performance.\n",
        "\n",
        "<b> Instructions </b>\n",
        "1. Plagiarism is strictly prohibited.\n",
        "2. Delayed submissions are not accepted\n",
        "3. Please DO NOT use any machine learning libraries unless and otherwise specified.\n",
        "\n",
        "\n",
        "\n",
        "<b> Part(a): Synthetic data generation </b>  \n",
        "\n",
        "1. Consider four bivariate Gaussians with means at (0,0), (0,1), (1,0), and (1,1), each with a diagonal covariance matrix with diagonal elements being 0.1. Sample 90 points from each Gaussian, resulting in a total of 360 points. Allocate 30 points from each Gaussian for training and 60 points for testing. This results in a total of 120 points for training and 240 points for testing.\n",
        "\n",
        "2. Create a 2-class training set ($[X_{train}, Y_{train}]$) and a test set ($[X_{test}, Y_{test}]$) by labeling the data sampled from Gaussians with means at (0,0) and (1,1) as class 1, and the data sampled from Gaussians with means at (0,1) and (1,0) as class 2. Assign a label of +1 to class1 and -1 to class2.\n",
        "\n",
        "4. Visualize both train and test sets using the scatter plot on a 2-D plane. Indicate the data points from class 1 with a green color and those from class 2 with a blue color.\n",
        "\n",
        "<b> Part(b): KNN Classification - </b> The k-Nearest Neighbors (KNN) classifier algorithm is a straightforward yet powerful tool for classification tasks. The KNN classifier takes the test data point, computes distances to all points in the training set, identifies the 'k' nearest neighbors based on these distances, and assigns the test data to the class that the majority of its neighbors belong to.\n",
        "\n",
        "<b> Programming questions </b>\n",
        "\n",
        "\n",
        "1. Develop a Python class named 'KNeighbourClassifier' that encapsulates the hyperparameters of a K-Nearest Neighbors (KNN) classifier, such as the number of neighbors (k) and the distance metric. Implement the following methods within this class. $\\textit{fit(X_train, y_train):}$ This method should accept training data (X_train) and corresponding labels (y_train) as input. It should store these inputs as internal attributes of the class for later use in prediction. $\\textit{predict(X_test):} This method should take test data (X_test) as input and return the predicted labels for the given data points using the KNN algorithm.\n",
        "\n",
        "2. Instantiate an object of the KNeighbourClassifier class. Train the classifier on the provided training data by calling the fit() method. Use this trained classifier to predict the labels of the test data. Finally, evaluate the classifier's accuracy by comparing the predicted labels with the actual (original) labels of the test data\n",
        "\n",
        "3. Generate a 2D scatter plot of the test data, assigning green to class 1, blue to class 2, and red to test data points whose predicted labels do not match their true labels.\n",
        "\n",
        "4. Visualize the decision boundaries of a given classifier by generating a 2D grid and predicting the class labels for each point in the grid using the KNN algorithm. Color-code the grid points based on their predicted class labels to visually represent the decision regions\n",
        "\n",
        "<b> Part(c): Parameter selection: What is good value for k? - </b> A common approach to finding the optimal value for the hyperparameter 'k' in K-Nearest Neighbors is through cross-validation. In this method, a portion (ρ%) of the training data is set aside as a validation set. The KNN model is then trained on the remaining data and evaluated on the validation set for a range of 'k' values. The optimal 'k' is selected as the value that results in the best performance on the validation set.\n",
        "\n",
        "<b> Algorithm </b>\n",
        "\n",
        "1. Perform hold-out cross-validation by setting aside a fraction (ρ of the training set for validation. Note: You may use ρ = 0.3, and repeat the procedure 10 times. The hold-out procedure may be quite unstable.\n",
        "2. Use a large range of candidate values for k (e.g. k = 1, 3, 5..., 21). Notice odd numbers are considered to avoid ties.\n",
        "3. Repeat the process for 10 times using a random cross-validation set each time with a ρ = 0.3.\n",
        "4. Plot the training and validation errors for the different values of k.\n",
        "\n",
        "<b> Questions </b>\n",
        "\n",
        "5. How would you now answer the question \"what is the best value for k\"?\n",
        "6. How is the value of k affected by ρ (percentage of points held out) and number of repetitions? What does a large number of repetitions provide?\n",
        "7. Apply the model obtained by cross-validation (i.e., best k) to the test set and check if there is an improvement on the classification error over the result of Part (b).\n",
        "\n",
        "<b> Part(d): Influence of training data on KNN classifier - </b>\n",
        "\n",
        "1. Evaluate the performance as the size of the training set\n",
        "grows, e.g., n = {200, 400, 1200,...}. How would you choose a good range for k as n changes? What can you say about the stability of the solution? Check by repeating the validation multiple times.\n",
        "\n",
        "2. Investigate how the distribution of the training data affects the performance of the KNN algorithm. Replace the Gaussian distribution used in part (a) with a Laplacian distribution. Generate both training and testing datasets based on these distributions. Evaluate the KNN classifier's performance on both datasets. Analyze whether the KNN achieves comparable performance with both Gaussian and Laplacian distributed data. Examine the effect of changing the distance metric from $l_{2}$ to $l_{1}$ on the KNN performance for both Gaussian and Laplacian distributed datasets\n",
        "\n",
        "\n",
        "<b> Part(e): What is the influence of distance measure on decision regions ? - </b>\n",
        "\n",
        "1. Evaluate the performance of the KNN classifier with different distance measures such as $l_{1}$, $l_{2}$, etc,.\n",
        "2. Plot the decision regions of the KNN classifier with different distance measures.\n",
        "3. Report your observations.  \n",
        "\n",
        "<b> Part(f): MNIST Digit classification using KNN classifier: </b> :\n",
        "\n",
        "1. Use the above written KNN classifier to perform digit claissification using MNIST digit dataset.\n",
        "2. The MNIST dataset consists of approximately 70,000 images of handwritten digits. Create training, validation, and test datasets from this entire dataset with the respective proportions of 80%, 10%, and 10%. The 28x28 images in the MNIST data set may be flattened to arrive at a 784 dimensional vector.\n",
        "3. Use the most suitable distance metric and k to maximize the test data performance.\n",
        "4. Create a confusion matrix to understand the most confused classes (digits).\n",
        "5. Suggest alternate ways to improve the performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#All imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "metadata": {
        "id": "gCAeXbdvgFVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Part(a) #Synthetic data generation\n",
        "########################################\n",
        "#Define means and covariances\n",
        "mean1=\n",
        "mean2=\n",
        "mean3=\n",
        "mean4=\n",
        "cov=\n",
        "\n",
        "#Sample data points from the bivariate Gaussian distribution\n",
        "#You can use \"np.random.multivariate_normal\" function to sample the data points from the multivariate Gaussian distribution\n",
        "\n",
        "\n",
        "\n",
        "#Generate training data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Generate testing data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Visualize the data using plt.scatter() function\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjgkoGQOEjis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#Part(b) #KNN classification\n",
        "########################################\n",
        "\n",
        "#Write \"kNNClassify\" function\n",
        "class KNeighborsClassifier:\n",
        "    \"\"\"\n",
        "    K-Nearest Neighbors Classifier\n",
        "    This class implements the k-nearest neighbors algorithm for classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_neighbors=5,distance_metric=\"l2\"):\n",
        "        #Initializes the KNeighborsClassifier with the specified number of neighbors. Defaults to 1.\n",
        "        #Default distance_metric is L2 norm\n",
        "\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.distance_metric = distance_metric\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(X_train,Y_train):\n",
        "        #This method fits the k-nearest neighbors classifier from the training data. X_train is the training data, represented as a numpy array of shape (n_samples, n_features), and Y_train is the target values of the training data, represented as a numpy array of shape (n_samples,)\n",
        "\n",
        "\n",
        "    def predict(X_test):\n",
        "        #This method predicts the class labels for a set of data samples. X_test is the data to be predicted, represented as a numpy array of shape (n_samples, n_features)\n",
        "\n",
        "\n",
        "\n",
        "#Write \"KNNAccuracy\" function\n",
        "def KNNAccuracy(true,pred):\n",
        "    #Inputs : Ground truth and predicted labels\n",
        "    #Outputs : Portion of data points that are correctly classified, i.e., accuracy\n",
        "\n",
        "\n",
        "\n",
        "#Create a visual representation of predictions\n",
        "\n",
        "\n",
        "\n",
        "#Generate and visualize the decision regions and overlay the test points\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pH91uokhDV_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "#Part(c): Parameter selection: What is good value for k?\n",
        "####################################\n",
        "#Write holdoutCVkNN() Function\n",
        "def holdoutCVkNN(k_range,rho):\n",
        "  #Iterate through range of k values\n",
        "    #Hold out rho fraction of training data in each repetition.\n",
        "\n",
        "\n",
        "\n",
        "  #Return errors on training and validation data\n",
        "\n",
        "\n",
        "\n",
        "#Plot training and validation errors for different values of k\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#what is the best value for k?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Effect of rho on k\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Evaluate the performance on test set with the best hyper parameters ( i.e best k ).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EWmRWE8pDgnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#Part(d): Influence of training data on KNN classifier\n",
        "##################################\n",
        "#Performance evaluation as n increases\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Performance evaluation with different data distributions, i.e., Gaussian, Laplacian\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PG8vSBuoEEYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#Part(e): Influence of distance metric on KNN classifier\n",
        "##################################\n",
        "#Performance evaluation of KNN classifier with different distance metric\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Understand the decision regions of KNN classifier with different distance metric\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xUub5YBDEfX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#Part(f): MNIST Digit classification using KNN classifier\n",
        "##################################\n",
        "\n",
        "#Load MNIST data\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784')\n",
        "images = mnist.data.to_numpy()\n",
        "targets = mnist.target.to_numpy()\n",
        "#Plot a few images\n",
        "plt.subplot(211)\n",
        "plt.imshow((images[0].reshape(28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.subplot(212)\n",
        "plt.imshow(images[1].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "\n",
        "#Create train, validation and test splits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Write 'MultiClassKNNClassify' function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Empirically chose most suitable k and distance metric based on the evauation on cross-validation data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Evaluate the performance on test data with the best hyper parameters ( k, distance metric ) obtained from cross validation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Create a confusion matrix for test data\n",
        "def compute_confusion_matrix(true, pred):\n",
        "    #Inputs: Ground truth labels and classifier predictions\n",
        "    #Outputs: Confusion matrix\n",
        "    #Confusion matrix: A confusion matrix is a table that summarizes the performance of a classification model by comparing its predictions to the actual ground truth values\n",
        "    #Rows: Represent the actual class labels\n",
        "    #Columns: Represent the predicted class labels\n",
        "\n",
        "\n",
        "\n",
        "#Suggest an alternative ways to improve performance\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l1atXB22I7va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Report your observations </b>\n",
        "1. Write down the best accuracy on synthetic test data generated from Gaussian distribution\n",
        "\n",
        "2. Write down the best accuracy on MNIST validation and test data.\n",
        "\n",
        "3. Report your observations on the confusion matrix of KNN classifier on MNIST test data\n"
      ],
      "metadata": {
        "id": "OohdgUOoAenj"
      }
    }
  ]
}